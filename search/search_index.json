{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the NewPipe Documentaiton.\n\n\nThis side is a beginner friendly tutorial and documentatin for people who want to use, or write services for the \nNewPipe Extractor\n.\nThis however is an addition to our \njdoc documentation\n.\n\n\nThis tutorial and the documentation are in an early state. So \nfeedback\n is always welcome :D\n\n\nStay Tuned\n\n\nTune yourself to stay ;D",
            "title": "Welcome to the NewPipe Documentaiton."
        },
        {
            "location": "/#welcome-to-the-newpipe-documentaiton",
            "text": "This side is a beginner friendly tutorial and documentatin for people who want to use, or write services for the  NewPipe Extractor .\nThis however is an addition to our  jdoc documentation .  This tutorial and the documentation are in an early state. So  feedback  is always welcome :D",
            "title": "Welcome to the NewPipe Documentaiton."
        },
        {
            "location": "/#stay-tuned",
            "text": "Tune yourself to stay ;D",
            "title": "Stay Tuned"
        },
        {
            "location": "/00_Prepare_everything/",
            "text": "Prepare everything\n\n\nWelcome to the NewPipe tutorial. This tutorial will guide you through the process of creating your own NewPipeExtractor\nservice with which NewPipe will gain support for a dedicated streaming service like YouTube, Vimeo or SournCloud. Let's\ndive right. ;D\n\n\nSetup your dev environment\n\n\nFirst and foremost you need to meet certain conditions in order to write your own service.\n\n\nWhat you need to know\n\n\n\n\nBasic understanding of \ngit\n\n\nGood \nJava\n knowledge\n\n\nGood understanding of \nweb technology\n\n\nBasic understanding about \nunit testing\n and \nJUnit\n\n\nFlawless understanding of how to \ncontribute\n to the \nNewPipe project\n\n\n\n\nWhat you need to have\n\n\n\n\nA dev environment/ide that supports:\n\n\ngit\n\n\njava 8\n\n\ngradle\n\n\nI highly recomend \nIDEA Community\n since it has everything we need.\n\n\n\n\n\n\nA \ngithub\n account\n\n\nA loot of patience and excitement ;D\n\n\n\n\nAfter making sure all these conditions are provided fork the \nNewPipeExtractor\n,\nusing the \nfork button\n.\nThis way you have your own working repository. Now clone this repository into your local folder in which you want to work in.\nNext import the cloned project into your \nide\n\nand \nrun\n it.\nIf all the checks are green you did everything right, and you are good to go to move on to the next chapter.\n\n\n\nInclusion criteria for services\n\n\nAfter creating you own service you will need to submit it to our \nNewPipeExtractor\n\n repository. However in order to include your changes you need to follow these rules:\n\n\n\n\nStick to our \nCode contribution guidelines\n\n\nDo not send services that present content we \ndon't allow\n on NewPipe.\n\n\nYou need to be willing to keep on maintaining your service after submission.\n\n\nBe patient and do the requested changes when one of our maintainers rejects your code.\n\n\n\n\nAllowed Content\n\n\n\n\nBasically anything except \nNOT allowed content\n.\n\n\nAny kind of porn/NSFW that is allowed according to the \nUS Porn act\n.\n\n\nAdvertisement (may be handled specially tho)\n\n\n\n\nNOT allowed Content\n\n\n\n\nNSFL\n\n\nPorn that is not allowed according to \nUS Porn act\n.\n\n\nAny form of violence\n\n\nChild pornography\n\n\nMedia that harms others\n\n\nMedia that shows the violation of human rights\n\n\nCopyright infringement/pirated media",
            "title": "Prepare everything"
        },
        {
            "location": "/00_Prepare_everything/#prepare-everything",
            "text": "Welcome to the NewPipe tutorial. This tutorial will guide you through the process of creating your own NewPipeExtractor\nservice with which NewPipe will gain support for a dedicated streaming service like YouTube, Vimeo or SournCloud. Let's\ndive right. ;D",
            "title": "Prepare everything"
        },
        {
            "location": "/00_Prepare_everything/#setup-your-dev-environment",
            "text": "First and foremost you need to meet certain conditions in order to write your own service.",
            "title": "Setup your dev environment"
        },
        {
            "location": "/00_Prepare_everything/#what-you-need-to-know",
            "text": "Basic understanding of  git  Good  Java  knowledge  Good understanding of  web technology  Basic understanding about  unit testing  and  JUnit  Flawless understanding of how to  contribute  to the  NewPipe project",
            "title": "What you need to know"
        },
        {
            "location": "/00_Prepare_everything/#what-you-need-to-have",
            "text": "A dev environment/ide that supports:  git  java 8  gradle  I highly recomend  IDEA Community  since it has everything we need.    A  github  account  A loot of patience and excitement ;D   After making sure all these conditions are provided fork the  NewPipeExtractor ,\nusing the  fork button .\nThis way you have your own working repository. Now clone this repository into your local folder in which you want to work in.\nNext import the cloned project into your  ide \nand  run  it.\nIf all the checks are green you did everything right, and you are good to go to move on to the next chapter.",
            "title": "What you need to have"
        },
        {
            "location": "/00_Prepare_everything/#inclusion-criteria-for-services",
            "text": "After creating you own service you will need to submit it to our  NewPipeExtractor \n repository. However in order to include your changes you need to follow these rules:   Stick to our  Code contribution guidelines  Do not send services that present content we  don't allow  on NewPipe.  You need to be willing to keep on maintaining your service after submission.  Be patient and do the requested changes when one of our maintainers rejects your code.",
            "title": "Inclusion criteria for services"
        },
        {
            "location": "/00_Prepare_everything/#allowed-content",
            "text": "Basically anything except  NOT allowed content .  Any kind of porn/NSFW that is allowed according to the  US Porn act .  Advertisement (may be handled specially tho)",
            "title": "Allowed Content"
        },
        {
            "location": "/00_Prepare_everything/#not-allowed-content",
            "text": "NSFL  Porn that is not allowed according to  US Porn act .  Any form of violence  Child pornography  Media that harms others  Media that shows the violation of human rights  Copyright infringement/pirated media",
            "title": "NOT allowed Content"
        },
        {
            "location": "/01_Concept_of_the_extractor/",
            "text": "Concept of the Extractor\n\n\nCollector/Extractor pattern\n\n\nBefore we can start coding our own service we need to understand the basic concept of the extractor. There is a pattern\nyou will find all over the code. It is called the \nextractor/collector\n pattern. The idea behind this pattern is that\nthe \nextractor\n\nwould produce single peaces of data, and the collector would take it and form usable data for the front end out of it.\nThe collector also controls the parsing process, and takes care about error handling. So if the extractor fails at any\npoint the collector will decide whether it should continue parsing or not. This requires the extractor to be made out of\nmany small methods. One method for every data field the collector wants to have. The collectors are provided by NewPipe.\nYou need to take care of the extractors.\n\n\nUsage in the front end\n\n\nSo typical call for retrieving data from a website would look like this:\n\n\nInfo info;\ntry {\n    // Create a new Extractor with a given context provided as parameter.\n    Extractor extractor = new Extractor(some_meta_info);\n    // Retrieves the data form extractor and builds info package.\n    info = Info.getInfo(extractor);\n} catch(Exception e) {\n    // handle errors when collector decided to break up extraction\n}\n\n\n\n\nTypical implementation of a single data extractor\n\n\nThe typical implementation of a single data extractor on the other hand would look like this:\n\n\nclass MyExtractor extends FutureExtractor {\n\n    public MyExtractor(RequiredInfo requiredInfo, ForExtraction forExtraction) {\n        super(requiredInfo, forExtraction);\n\n        ...\n    }\n\n    @Override\n    public void fetch() {\n        // Actually fetch the page data here\n    }\n\n    @Override\n    public String someDataFiled() \n        throws ExtractionException {    //The exception needs to be thrown if someting failed\n        // get piece of information and return it\n    }\n\n    ...                                 // More datafields\n}\n\n\n\n\nCollector/Extractor pattern for lists\n\n\nSometimes information can be represented as a list. In NewPipe a list is represented by a\n\nInfoItemsCollector\n.\nA InfoItemCollector will collect and assemble a list of \nInfoItem\n.\nFor each item that should be extracted a new Extractor must be created, and given to the InfoItemCollector via \ncommit()\n.\n\n\n\n\nIf you are implementing a list for your service you need to extend InfoItem containing the extracted information,\nand implement an \nInfoItemExtractor\n\nthat will return the data of one InfoItem.\n\n\nA common Implementation would look like this:\n\n\nprivate MyInfoItemCollector collectInfoItemsFromElement(Element e) {\n    MyInfoItemCollector collector = new MyInfoItemCollector(getServiceId());\n\n    for(final Element li : element.children()) {\n        collector.commit(new InfoItemExtractor() {\n            @Override\n            public String getName() throws ParsingException {\n                ...\n            }\n\n            @Override\n            public String getUrl() throws ParsingException {\n                ...\n            }\n\n            ...\n    }\n    return collector;\n}\n\n\n\n\n\nInfoItems encapsulated in pages\n\n\nWhen a streaming site shows a list of items it usually offers some additional information about that list, like it's title a thumbnail\nor its creator. Such info can be called \nlist header\n.\n\n\nWhen a website shows a long list of items it usually does not load the whole list, but only a part of it. In order to get more items you may have to click on a next page button, or scroll down. \n\n\nThis is why a list in NewPipe lists are chopped down into smaller lists called \nInfoItemsPage\ns. Each page has its own URL, and needs to be extracted separately.\n\n\nAdditional metainformation about the list such as it's title a thumbnail\nor its creator, and extracting multiple pages can be handled by a\n\nListExtractor\n,\nand it's \nListExtractor.InfoItemsPage\n.\n\n\nFor extracting list header information it behaves like a regular extractor. For handling \nInfoItemsPages\n it adds methods\nsuch as:\n\n\n\n\ngetInitialPage()\n\n   which will return the first page of InfoItems.\n\n\ngetNextPageUrl()\n\n   If a second Page of InfoItems is available this will return the URL pointing to them.\n\n\ngetPage()\n\n   returns a ListExtractor.InfoItemsPage by its URL which was retrieved by the \ngetNextPageUrl()\n method of the previous page.\n\n\n\n\nThe reason why the first page is handled speciall is because many Websites such as Youtube will load the first page of\nitems like a regular webpage, but all the others as AJAX request.",
            "title": "Concept of the Extractor"
        },
        {
            "location": "/01_Concept_of_the_extractor/#concept-of-the-extractor",
            "text": "",
            "title": "Concept of the Extractor"
        },
        {
            "location": "/01_Concept_of_the_extractor/#collectorextractor-pattern",
            "text": "Before we can start coding our own service we need to understand the basic concept of the extractor. There is a pattern\nyou will find all over the code. It is called the  extractor/collector  pattern. The idea behind this pattern is that\nthe  extractor \nwould produce single peaces of data, and the collector would take it and form usable data for the front end out of it.\nThe collector also controls the parsing process, and takes care about error handling. So if the extractor fails at any\npoint the collector will decide whether it should continue parsing or not. This requires the extractor to be made out of\nmany small methods. One method for every data field the collector wants to have. The collectors are provided by NewPipe.\nYou need to take care of the extractors.",
            "title": "Collector/Extractor pattern"
        },
        {
            "location": "/01_Concept_of_the_extractor/#usage-in-the-front-end",
            "text": "So typical call for retrieving data from a website would look like this:  Info info;\ntry {\n    // Create a new Extractor with a given context provided as parameter.\n    Extractor extractor = new Extractor(some_meta_info);\n    // Retrieves the data form extractor and builds info package.\n    info = Info.getInfo(extractor);\n} catch(Exception e) {\n    // handle errors when collector decided to break up extraction\n}",
            "title": "Usage in the front end"
        },
        {
            "location": "/01_Concept_of_the_extractor/#typical-implementation-of-a-single-data-extractor",
            "text": "The typical implementation of a single data extractor on the other hand would look like this:  class MyExtractor extends FutureExtractor {\n\n    public MyExtractor(RequiredInfo requiredInfo, ForExtraction forExtraction) {\n        super(requiredInfo, forExtraction);\n\n        ...\n    }\n\n    @Override\n    public void fetch() {\n        // Actually fetch the page data here\n    }\n\n    @Override\n    public String someDataFiled() \n        throws ExtractionException {    //The exception needs to be thrown if someting failed\n        // get piece of information and return it\n    }\n\n    ...                                 // More datafields\n}",
            "title": "Typical implementation of a single data extractor"
        },
        {
            "location": "/01_Concept_of_the_extractor/#collectorextractor-pattern-for-lists",
            "text": "Sometimes information can be represented as a list. In NewPipe a list is represented by a InfoItemsCollector .\nA InfoItemCollector will collect and assemble a list of  InfoItem .\nFor each item that should be extracted a new Extractor must be created, and given to the InfoItemCollector via  commit() .   If you are implementing a list for your service you need to extend InfoItem containing the extracted information,\nand implement an  InfoItemExtractor \nthat will return the data of one InfoItem.  A common Implementation would look like this:  private MyInfoItemCollector collectInfoItemsFromElement(Element e) {\n    MyInfoItemCollector collector = new MyInfoItemCollector(getServiceId());\n\n    for(final Element li : element.children()) {\n        collector.commit(new InfoItemExtractor() {\n            @Override\n            public String getName() throws ParsingException {\n                ...\n            }\n\n            @Override\n            public String getUrl() throws ParsingException {\n                ...\n            }\n\n            ...\n    }\n    return collector;\n}",
            "title": "Collector/Extractor pattern for lists"
        },
        {
            "location": "/01_Concept_of_the_extractor/#infoitems-encapsulated-in-pages",
            "text": "When a streaming site shows a list of items it usually offers some additional information about that list, like it's title a thumbnail\nor its creator. Such info can be called  list header .  When a website shows a long list of items it usually does not load the whole list, but only a part of it. In order to get more items you may have to click on a next page button, or scroll down.   This is why a list in NewPipe lists are chopped down into smaller lists called  InfoItemsPage s. Each page has its own URL, and needs to be extracted separately.  Additional metainformation about the list such as it's title a thumbnail\nor its creator, and extracting multiple pages can be handled by a ListExtractor ,\nand it's  ListExtractor.InfoItemsPage .  For extracting list header information it behaves like a regular extractor. For handling  InfoItemsPages  it adds methods\nsuch as:   getInitialPage() \n   which will return the first page of InfoItems.  getNextPageUrl() \n   If a second Page of InfoItems is available this will return the URL pointing to them.  getPage() \n   returns a ListExtractor.InfoItemsPage by its URL which was retrieved by the  getNextPageUrl()  method of the previous page.   The reason why the first page is handled speciall is because many Websites such as Youtube will load the first page of\nitems like a regular webpage, but all the others as AJAX request.",
            "title": "InfoItems encapsulated in pages"
        }
    ]
}